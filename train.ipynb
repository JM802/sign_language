{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. ÂàõÂª∫Ê∫êÁ†ÅÁõÆÂΩï\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "sys.path.append(\"/kaggle/working/src\")\n",
    "\n",
    "# 2. Ëá™Âä®ÂØªÊâæÊï∞ÊçÆÊ†πÁõÆÂΩï\n",
    "DATA_ROOT = \"\"\n",
    "# ÈÅçÂéÜ input ÁõÆÂΩïÂØªÊâæÁâπÂæÅÊ†áÂøóÊñá‰ª∂\n",
    "for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "    if \"train_map_300.txt\" in files:\n",
    "        DATA_ROOT = root\n",
    "        break\n",
    "\n",
    "if DATA_ROOT:\n",
    "    print(f\"‚úÖ ÊàêÂäüÂÆö‰ΩçÊï∞ÊçÆÊ†πÁõÆÂΩï: {DATA_ROOT}\")\n",
    "    print(\"   ÂåÖÂê´Êñá‰ª∂Á§∫‰æã:\", os.listdir(DATA_ROOT)[:5])\n",
    "    \n",
    "    # Ê£ÄÊü•ÂÖ≥ÈîÆÁöÑÂΩí‰∏ÄÂåñÊñá‰ª∂ÊòØÂê¶Â≠òÂú®\n",
    "    if \"global_mean_300_double_vel.npy\" in os.listdir(DATA_ROOT):\n",
    "        print(\"‚úÖ Ê£ÄÊµãÂà∞ÂèåÈáçÁõ∏ÂØπÂùêÊ†áÁªüËÆ°Èáè (Double Relative Stats)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Ë≠¶ÂëäÔºöÊú™ÊâæÂà∞ global_mean_300_double_vel.npyÔºåËØ∑Ê£ÄÊü•Êï∞ÊçÆÈõÜÔºÅ\")\n",
    "else:\n",
    "    print(\"‚ùå ÈîôËØØ: Êú™ÊâæÂà∞Êï∞ÊçÆÈõÜÔºåËØ∑Ê£ÄÊü•ÊòØÂê¶ Add Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea935ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/config.py\n",
    "import os\n",
    "\n",
    "# ================= Ë∑ØÂæÑÈÖçÁΩÆ =================\n",
    "RESULT_DIR = \"/kaggle/working/results\"\n",
    "\n",
    "# ================= Êï∞ÊçÆÂèÇÊï∞ =================\n",
    "# ÂéüÂßã MP ÁâπÂæÅÊòØ 134ÔºåÁªèËøáÂèåÈáçÁõ∏ÂØπ+ÈÄüÂ∫¶Â§ÑÁêÜÂêéÂèòÊàê 268\n",
    "INPUT_SIZE = 268     \n",
    "SEQ_LEN = 64         \n",
    "NUM_CLASSES = 300    \n",
    "\n",
    "# ================= ËÆ≠ÁªÉÂèÇÊï∞ =================\n",
    "BATCH_SIZE = 64      # P100 ÊòæÂ≠òË∂≥Â§ü\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a36392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        # MLP Attention: Linear -> Tanh -> Linear\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: [Batch, Seq, Hidden*2]\n",
    "        scores = self.attn(lstm_output)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(weights * lstm_output, dim=1)\n",
    "        return context\n",
    "\n",
    "class BiLSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, dropout=0.5):\n",
    "        super(BiLSTMAttentionModel, self).__init__()\n",
    "        \n",
    "        # 1. BiLSTM\n",
    "        # ‚ùó ‰øÆÂ§ç1: LSTM dropout Âº∫Âà∂ËÆæ‰∏∫ 0.2ÔºåÈò≤Ê≠¢Â±ÇÈó¥‰ø°ÊÅØ‰∏¢Â§±ËøáÂ§öÂØºËá¥Ê¢ØÂ∫¶Êñ≠Ë£Ç\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.2 \n",
    "        )\n",
    "        \n",
    "        # 2. LayerNorm\n",
    "        self.ln = nn.LayerNorm(hidden_size * 2)\n",
    "        \n",
    "        # 3. Attention\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "        # 4. Classifier Head\n",
    "        # ‚ùó ‰øÆÂ§ç2: Classifier dropout ËÆæ‰∏∫ 0.3ÔºåÈò≤Ê≠¢Ê¨†ÊãüÂêà\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.ln(out)\n",
    "        context = self.attention(out)\n",
    "        output = self.classifier(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747543f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# Ê†∏ÂøÉÁâπÂæÅÂ∑•Á®ãÔºöÂèåÈáçÁõ∏ÂØπÂùêÊ†á + ÈÄüÂ∫¶\n",
    "# ==========================================\n",
    "def feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Input: (T, 134) Absolute Coordinates\n",
    "    Output: (T, 268) Double Relative + Velocity\n",
    "    \"\"\"\n",
    "    T = data.shape[0]\n",
    "    \n",
    "    pose = data[:, 0:50].reshape(T, 25, 2)\n",
    "    lh   = data[:, 50:92].reshape(T, 21, 2)\n",
    "    rh   = data[:, 92:134].reshape(T, 21, 2)\n",
    "\n",
    "    nose = pose[:, 0:1, :]      \n",
    "    l_wrist = lh[:, 0:1, :]     \n",
    "    r_wrist = rh[:, 0:1, :]     \n",
    "\n",
    "    pose_rel = pose - nose\n",
    "    lh_rel = lh - l_wrist\n",
    "    rh_rel = rh - r_wrist\n",
    "\n",
    "    pose_d = np.diff(pose_rel, axis=0, prepend=pose_rel[:1])\n",
    "    lh_d   = np.diff(lh_rel,   axis=0, prepend=lh_rel[:1])\n",
    "    rh_d   = np.diff(rh_rel,   axis=0, prepend=rh_rel[:1])\n",
    "\n",
    "    final_feat = np.concatenate([\n",
    "        pose_rel.reshape(T, -1),\n",
    "        lh_rel.reshape(T, -1),\n",
    "        rh_rel.reshape(T, -1),\n",
    "        pose_d.reshape(T, -1),\n",
    "        lh_d.reshape(T, -1),\n",
    "        rh_d.reshape(T, -1)\n",
    "    ], axis=1)\n",
    "\n",
    "    return final_feat\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, map_file, data_root, mode='train', seq_len=64):\n",
    "        self.mode = mode\n",
    "        self.data_root = data_root\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        with open(map_file, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "            \n",
    "        mean_path = os.path.join(data_root, \"global_mean_300_double_vel.npy\")\n",
    "        std_path = os.path.join(data_root, \"global_std_300_double_vel.npy\")\n",
    "        \n",
    "        # Ë∑ØÂæÑÂÆπÈîô\n",
    "        if not os.path.exists(mean_path):\n",
    "             for root, dirs, files in os.walk(data_root):\n",
    "                if \"global_mean_300_double_vel.npy\" in files:\n",
    "                    mean_path = os.path.join(root, \"global_mean_300_double_vel.npy\")\n",
    "                    std_path = os.path.join(root, \"global_std_300_double_vel.npy\")\n",
    "                    break\n",
    "\n",
    "        if os.path.exists(mean_path):\n",
    "            self.mean = np.load(mean_path).astype(np.float32)\n",
    "            self.std = np.load(std_path).astype(np.float32)\n",
    "        else:\n",
    "            self.mean = None\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ‚ùó ‰øÆÂ§ç4: Èò≤Ê≠¢Êó†ÈôêÈÄíÂΩíÊ≠ªÂæ™ÁéØÔºåÂ∞ùËØïÊúÄÂ§ö 10 Ê¨°\n",
    "        max_retries = 10\n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                return self._load_item(idx)\n",
    "            except Exception:\n",
    "                # Â¶ÇÊûúËØªÂèñÂ§±Ë¥•ÔºåÂ∞ùËØï‰∏ã‰∏Ä‰∏™Á¥¢Âºï\n",
    "                idx = (idx + 1) % len(self)\n",
    "        \n",
    "        # Â¶ÇÊûúËøûÁª≠ 10 ‰∏™ÈÉΩÂùè‰∫ÜÔºåËøîÂõûÂÖ® 0 Tensor (Fallback)\n",
    "        return torch.zeros((self.seq_len, 268), dtype=torch.float32), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "    def _load_item(self, idx):\n",
    "        line = self.lines[idx].strip()\n",
    "        original_path, label = line.split(',')\n",
    "        filename = os.path.basename(original_path)\n",
    "        \n",
    "        npy_path = os.path.join(self.data_root, \"processed_features_300\", filename)\n",
    "        if not os.path.exists(npy_path):\n",
    "            found = False\n",
    "            for root, dirs, files in os.walk(self.data_root):\n",
    "                if filename in files:\n",
    "                    npy_path = os.path.join(root, filename)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                raise FileNotFoundError(f\"File not found: {filename}\")\n",
    "\n",
    "        # 1. Âä†ËΩΩ\n",
    "        raw_data = np.load(npy_path).astype(np.float32)\n",
    "        \n",
    "        # 2. ÁâπÂæÅÂ∑•Á®ã (134 -> 268)\n",
    "        data = feature_engineering(raw_data)\n",
    "        \n",
    "        # 3. ÂΩí‰∏ÄÂåñ\n",
    "        if self.mean is not None:\n",
    "            data = (data - self.mean) / self.std\n",
    "            \n",
    "        # ‚ùó ‰øÆÂ§ç3: Âô™Â£∞ÂøÖÈ°ªÂú®ÂΩí‰∏ÄÂåñ‰πãÂêéÂä†Ôºå‰∏îÂº∫Â∫¶ËÆæ‰∏∫ 0.02\n",
    "        if self.mode == 'train':\n",
    "            noise = np.random.normal(0, 0.02, data.shape).astype(np.float32)\n",
    "            data += noise\n",
    "        \n",
    "        # 4. ÈïøÂ∫¶Áªü‰∏Ä\n",
    "        curr_len = data.shape[0]\n",
    "        feature_dim = data.shape[1]\n",
    "        if curr_len > self.seq_len:\n",
    "            indices = np.linspace(0, curr_len-1, self.seq_len, dtype=int)\n",
    "            data = data[indices]\n",
    "        elif curr_len < self.seq_len:\n",
    "            pad = np.zeros((self.seq_len - curr_len, feature_dim), dtype=np.float32)\n",
    "            data = np.concatenate([data, pad], axis=0)\n",
    "\n",
    "        return torch.from_numpy(data), torch.tensor(int(label), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c26b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from dataset import WLASLDataset\n",
    "from model import BiLSTMAttentionModel\n",
    "import config\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Worker ÂàùÂßãÂåñ\n",
    "def worker_init_fn(worker_id):\n",
    "    seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def plot_results(history, save_dir):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, \"training_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_topk(model, dataloader, device, k=5):\n",
    "    model.eval()\n",
    "    correct_1 = 0\n",
    "    correct_k = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            out = model(x)\n",
    "            \n",
    "            _, pred_1 = torch.max(out, 1)\n",
    "            correct_1 += (pred_1 == y).sum().item()\n",
    "            \n",
    "            _, pred_k = out.topk(k, 1, True, True)\n",
    "            correct_k += pred_k.eq(y.view(-1, 1).expand_as(pred_k)).sum().item()\n",
    "            \n",
    "            total += y.size(0)\n",
    "            all_preds.extend(pred_1.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            \n",
    "    return correct_1/total, correct_k/total, all_labels, all_preds\n",
    "\n",
    "def main(data_root):\n",
    "    os.makedirs(config.RESULT_DIR, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üî• Device: {device}\")\n",
    "    \n",
    "    print(f\"üì¶ Loading Data from: {data_root}\")\n",
    "    train_ds = WLASLDataset(os.path.join(data_root, \"train_map_300.txt\"), data_root, 'train', config.SEQ_LEN)\n",
    "    val_ds = WLASLDataset(os.path.join(data_root, \"val_map_300.txt\"), data_root, 'val', config.SEQ_LEN)\n",
    "    test_ds = WLASLDataset(os.path.join(data_root, \"test_map_300.txt\"), data_root, 'test', config.SEQ_LEN)\n",
    "    \n",
    "    # ‚ùó ‰øÆÂ§ç5: ÊâÄÊúâ DataLoader ÈÉΩÂä†‰∏ä worker_init_fn\n",
    "    loader_kwargs = {\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"num_workers\": 4,  # Kaggle P100 ‰∏ä 4 ÊØîËæÉÁ®≥ÔºåÂ¶ÇÊûúÂç°Ê≠ªÊîπÊàê 2\n",
    "        \"pin_memory\": True,\n",
    "        \"persistent_workers\": True,\n",
    "        \"worker_init_fn\": worker_init_fn\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, shuffle=True, **loader_kwargs)\n",
    "    val_loader = DataLoader(val_ds, shuffle=False, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_ds, shuffle=False, **loader_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ Data Loaded: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n",
    "    \n",
    "    model = BiLSTMAttentionModel(\n",
    "        config.INPUT_SIZE, \n",
    "        hidden_size=512, \n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        dropout=0.5 # Ëøô‰∏™ÂèÇÊï∞Âú® model.py ÈáåËôΩÁÑ∂‰º†ËøõÂéª‰∫ÜÔºå‰ΩÜ model.py ÂÜÖÈÉ®Â∑≤Áªè‰øÆÊ≠£‰∏∫ 0.2 Âíå 0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    save_path = os.path.join(config.RESULT_DIR, \"best_model_300.pth\")\n",
    "    \n",
    "    print(\"üöÄ Starting Training...\")\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\", leave=False):\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(out, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (pred == y).sum().item()\n",
    "            \n",
    "        train_acc = correct / total\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        val_acc, val_acc5, _, _ = evaluate_topk(model, val_loader, device, k=5)\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            # print(f\"   ‚≠ê New Best Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} | Train={train_acc:.2%} | Val={val_acc:.2%} (Top5={val_acc5:.2%})\")\n",
    "\n",
    "    # Final Test\n",
    "    print(\"\\nüß™ Final Evaluation on Test Set...\")\n",
    "    if os.path.exists(save_path):\n",
    "        # ‚ùó ‰øÆÂ§ç6: Âä†ËΩΩÊ®°ÂûãÊó∂Âä†‰∏ä map_locationÔºåÈò≤Ê≠¢ËÆæÂ§á‰∏ç‰∏ÄËá¥Êä•Èîô\n",
    "        model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "    \n",
    "    acc_1, acc_5, y_true, y_pred = evaluate_topk(model, test_loader, device, k=5)\n",
    "    print(f\"\\nüèÜ Final Test Results:\")\n",
    "    print(f\"   Top-1 Accuracy: {acc_1:.2%}\")\n",
    "    print(f\"   Top-5 Accuracy: {acc_5:.2%}\")\n",
    "    \n",
    "    # Save Results\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_csv(os.path.join(config.RESULT_DIR, \"classification_report.csv\"))\n",
    "    \n",
    "    plot_results(history, config.RESULT_DIR)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cm, cmap='Blues', xticklabels=False, yticklabels=False)\n",
    "    plt.savefig(os.path.join(config.RESULT_DIR, \"confusion_matrix.png\"))\n",
    "    \n",
    "    print(f\"‚úÖ All results saved to {config.RESULT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    root_arg = sys.argv[1] if len(sys.argv) > 1 else \".\"\n",
    "    main(root_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fe6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_ROOT:\n",
    "    !python src/train.py \"{DATA_ROOT}\"\n",
    "else:\n",
    "    print(\"‚ùå Error: DATA_ROOT not found. Run Cell 1 first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
