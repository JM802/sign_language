# 后续提升方向

### 一、重建3维的手部模型，将手语画面3维化(需要重新训练一个模型)

1. **使用开源的 ExPose或 S2HAND，将 WLASL 视频输入这些 SOTA 模型，提取每一帧的  MANO 参数 （姿态 $\theta$ 和 形状 $\beta$）(我选择S2HAND,轻型模型能部署在边缘端)**
2. **MANO：**

   **① 形状参数：$\beta$ (Shape Parameters)**

   * **维度 ：通常是一个 $10$ 维的向量。**
   * **作用 ：控制手部的 固有属性 。比如手指的长短、手掌的宽窄、整体的胖瘦。**
   * **原理 ：它是通过对大量不同人的手进行 PCA 得到的。$\beta = 0$ 代表平均手型，改变数值会沿主成分方向调整形状。**

   **② 姿态参数：$\theta$ (Pose Parameters)**

   * **维度 ：$48$ 维向量。**
   * **全球旋转 (Global Rotation) ：$3$ 维（通常用轴角法表示），控制手腕在空间中的指向。**
   * **手指关节 (Joint Rotations) ：$15$ 个关键关节，每个关节 $3$ 自由度（$15 \times 3 = 45$ 维）。**
   * **作用 ：控制手部的 动作 。比如握拳、指向、打手势等。**

   **③ 姿态修正项 (Pose-Corrective Blend Shapes)**

   **虽然这不是用户直接输入的参数，但它是 MANO 的精髓。模型会根据 $\theta$ 的变化，自动计算皮肤的次级变形，防止关节处出现纸片折叠的廉价感。**
3. **MANO输出：**

   **当你给 MANO 输入了 $\beta$ 和 $\theta$ 后，它会通过线性混合蒙皮（LBS）算法输出：**

* **3D 网格 (Mesh) ：包含 $778$ 个顶点和 $1538$ 个面片。**
* **关节位置 (Joints) ：$21$ 个手部关键点的 3D 坐标。**

4. ExPose(全身表达性人体重构)：

   - 不是直接对整张图进行回归，而是先定位身体，再根据身体姿态裁剪出高分辨率的手部和面部图像。
   - 巧妙地解决了局部（手/脸）与全局（身体）的衔接问题。
   - 输出丰富 ：

     - Body : 形状与大关节动作。
     - Face : 包含面部表情（FLAME 参数)
     - Hands : 包含精细的手指动作（MANO 参数）
5. S2HAND

- 核心背景
  在野外场景（如街道、室内）获取精准的 3D 手部 Ground Truth非常困难且昂贵。
- 技术亮点
  - **自监督学习** ：它不需要海量的 3D 坐标标注，而是利用 **一致性约束**
  - **可微渲染** ：这是 S2HAND 的灵魂。
    1. 网络预测 MANO 参数（**$\beta, \theta$**）。
    2. 通过可微渲染器将 3D 手部网格投影回 2D 图像。
    3. **计算损失** ：比较投影出的手部掩码（Mask）和颜色与原图的差异
    4. **多视图/深度一致性** ：通过利用图像内部的对称性和深度信息，在没有 3D 标签的情况下也能训练出效果不错的手部模型。

### 二、具体流程


### 三、前端

1. 增加鼓励的话语和对聋哑人心态的关心（聋哑人生活比较艰难，思想和正常人一样，但是在社会上受到一些歧视）
